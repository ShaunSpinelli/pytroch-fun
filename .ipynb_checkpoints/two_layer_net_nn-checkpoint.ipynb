{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "PyTorch: nn\n",
    "-----------\n",
    "\n",
    "A fully-connected ReLU network with one hidden layer, trained to predict y from x\n",
    "by minimizing squared Euclidean distance.\n",
    "\n",
    "This implementation uses the nn package from PyTorch to build the network.\n",
    "PyTorch autograd makes it easy to define computational graphs and take gradients,\n",
    "but raw autograd can be a bit too low-level for defining complex neural networks;\n",
    "this is where the nn package can help. The nn package defines a set of Modules,\n",
    "which you can think of as a neural network layer that has produces output from\n",
    "input and may have some trainable weights.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1.0137114524841309\n",
      "1 1.0135858058929443\n",
      "2 1.013458251953125\n",
      "3 1.0133311748504639\n",
      "4 1.0132057666778564\n",
      "5 1.0130784511566162\n",
      "6 1.0129516124725342\n",
      "7 1.0128253698349\n",
      "8 1.0126993656158447\n",
      "9 1.012573003768921\n",
      "10 1.0124465227127075\n",
      "11 1.0123196840286255\n",
      "12 1.0121941566467285\n",
      "13 1.0120679140090942\n",
      "14 1.0119407176971436\n",
      "15 1.0118153095245361\n",
      "16 1.011688470840454\n",
      "17 1.0115633010864258\n",
      "18 1.0114367008209229\n",
      "19 1.011310338973999\n",
      "20 1.0111844539642334\n",
      "21 1.0110580921173096\n",
      "22 1.0109338760375977\n",
      "23 1.0108063220977783\n",
      "24 1.010681390762329\n",
      "25 1.0105546712875366\n",
      "26 1.0104286670684814\n",
      "27 1.0103027820587158\n",
      "28 1.010176658630371\n",
      "29 1.0100510120391846\n",
      "30 1.0099258422851562\n",
      "31 1.0098000764846802\n",
      "32 1.009673833847046\n",
      "33 1.009547472000122\n",
      "34 1.009421706199646\n",
      "35 1.009296178817749\n",
      "36 1.0091707706451416\n",
      "37 1.0090446472167969\n",
      "38 1.0089197158813477\n",
      "39 1.0087929964065552\n",
      "40 1.0086677074432373\n",
      "41 1.0085426568984985\n",
      "42 1.0084171295166016\n",
      "43 1.0082911252975464\n",
      "44 1.0081666707992554\n",
      "45 1.0080410242080688\n",
      "46 1.0079156160354614\n",
      "47 1.0077908039093018\n",
      "48 1.0076649188995361\n",
      "49 1.007539987564087\n",
      "50 1.0074148178100586\n",
      "51 1.007289171218872\n",
      "52 1.0071643590927124\n",
      "53 1.0070385932922363\n",
      "54 1.0069141387939453\n",
      "55 1.006789207458496\n",
      "56 1.0066639184951782\n",
      "57 1.006538987159729\n",
      "58 1.0064138174057007\n",
      "59 1.0062885284423828\n",
      "60 1.0061650276184082\n",
      "61 1.006040334701538\n",
      "62 1.0059154033660889\n",
      "63 1.0057902336120605\n",
      "64 1.0056649446487427\n",
      "65 1.005541205406189\n",
      "66 1.0054152011871338\n",
      "67 1.0052915811538696\n",
      "68 1.0051658153533936\n",
      "69 1.0050418376922607\n",
      "70 1.0049164295196533\n",
      "71 1.0047924518585205\n",
      "72 1.0046675205230713\n",
      "73 1.004542589187622\n",
      "74 1.0044187307357788\n",
      "75 1.0042946338653564\n",
      "76 1.0041691064834595\n",
      "77 1.004044771194458\n",
      "78 1.003920555114746\n",
      "79 1.0037963390350342\n",
      "80 1.0036722421646118\n",
      "81 1.0035468339920044\n",
      "82 1.0034228563308716\n",
      "83 1.0032988786697388\n",
      "84 1.0031745433807373\n",
      "85 1.0030499696731567\n",
      "86 1.0029267072677612\n",
      "87 1.0028018951416016\n",
      "88 1.0026776790618896\n",
      "89 1.0025538206100464\n",
      "90 1.002429723739624\n",
      "91 1.0023057460784912\n",
      "92 1.002181053161621\n",
      "93 1.0020571947097778\n",
      "94 1.0019330978393555\n",
      "95 1.001809000968933\n",
      "96 1.0016846656799316\n",
      "97 1.0015614032745361\n",
      "98 1.0014365911483765\n",
      "99 1.00131356716156\n",
      "100 1.0011897087097168\n",
      "101 1.0010653734207153\n",
      "102 1.0009430646896362\n",
      "103 1.0008176565170288\n",
      "104 1.0006952285766602\n",
      "105 1.000570297241211\n",
      "106 1.0004470348358154\n",
      "107 1.000324010848999\n",
      "108 1.0002000331878662\n",
      "109 1.0000767707824707\n",
      "110 0.9999529719352722\n",
      "111 0.999829113483429\n",
      "112 0.9997061491012573\n",
      "113 0.9995829463005066\n",
      "114 0.9994587898254395\n",
      "115 0.999335765838623\n",
      "116 0.9992120862007141\n",
      "117 0.9990893602371216\n",
      "118 0.9989659190177917\n",
      "119 0.9988423585891724\n",
      "120 0.9987196922302246\n",
      "121 0.9985960125923157\n",
      "122 0.998473048210144\n",
      "123 0.9983493685722351\n",
      "124 0.9982267618179321\n",
      "125 0.9981032609939575\n",
      "126 0.997979998588562\n",
      "127 0.9978560209274292\n",
      "128 0.9977328181266785\n",
      "129 0.9976106882095337\n",
      "130 0.9974881410598755\n",
      "131 0.9973646998405457\n",
      "132 0.9972420930862427\n",
      "133 0.9971194267272949\n",
      "134 0.996995747089386\n",
      "135 0.9968725442886353\n",
      "136 0.996749758720398\n",
      "137 0.9966270327568054\n",
      "138 0.9965039491653442\n",
      "139 0.9963814616203308\n",
      "140 0.9962587356567383\n",
      "141 0.9961363673210144\n",
      "142 0.996012806892395\n",
      "143 0.9958902597427368\n",
      "144 0.9957687258720398\n",
      "145 0.9956443905830383\n",
      "146 0.9955218434333801\n",
      "147 0.9953996539115906\n",
      "148 0.9952777028083801\n",
      "149 0.9951547384262085\n",
      "150 0.9950324296951294\n",
      "151 0.994910717010498\n",
      "152 0.9947866201400757\n",
      "153 0.9946654438972473\n",
      "154 0.9945424199104309\n",
      "155 0.9944208264350891\n",
      "156 0.9942976236343384\n",
      "157 0.9941744804382324\n",
      "158 0.9940533638000488\n",
      "159 0.9939309358596802\n",
      "160 0.9938071966171265\n",
      "161 0.9936858415603638\n",
      "162 0.9935636520385742\n",
      "163 0.9934414625167847\n",
      "164 0.9933196306228638\n",
      "165 0.9931971430778503\n",
      "166 0.993074893951416\n",
      "167 0.9929531812667847\n",
      "168 0.9928299784660339\n",
      "169 0.9927083253860474\n",
      "170 0.9925869107246399\n",
      "171 0.9924647212028503\n",
      "172 0.9923433065414429\n",
      "173 0.9922211766242981\n",
      "174 0.9921005368232727\n",
      "175 0.9919781684875488\n",
      "176 0.9918562173843384\n",
      "177 0.9917343258857727\n",
      "178 0.991612434387207\n",
      "179 0.9914909601211548\n",
      "180 0.9913690686225891\n",
      "181 0.991247296333313\n",
      "182 0.9911263585090637\n",
      "183 0.991003692150116\n",
      "184 0.9908820986747742\n",
      "185 0.9907609224319458\n",
      "186 0.990639328956604\n",
      "187 0.9905174374580383\n",
      "188 0.9903968572616577\n",
      "189 0.9902752041816711\n",
      "190 0.9901531338691711\n",
      "191 0.990031361579895\n",
      "192 0.9899106025695801\n",
      "193 0.9897893071174622\n",
      "194 0.9896683692932129\n",
      "195 0.9895469546318054\n",
      "196 0.989425778388977\n",
      "197 0.9893049001693726\n",
      "198 0.9891839027404785\n",
      "199 0.9890630841255188\n",
      "200 0.9889426231384277\n",
      "201 0.9888212084770203\n",
      "202 0.988700270652771\n",
      "203 0.9885790944099426\n",
      "204 0.9884589314460754\n",
      "205 0.9883379936218262\n",
      "206 0.9882171750068665\n",
      "207 0.9880967140197754\n",
      "208 0.9879757761955261\n",
      "209 0.9878552556037903\n",
      "210 0.9877339601516724\n",
      "211 0.9876136779785156\n",
      "212 0.987492561340332\n",
      "213 0.987372100353241\n",
      "214 0.9872517585754395\n",
      "215 0.9871314167976379\n",
      "216 0.9870103001594543\n",
      "217 0.9868899583816528\n",
      "218 0.9867693781852722\n",
      "219 0.9866488575935364\n",
      "220 0.986528217792511\n",
      "221 0.9864083528518677\n",
      "222 0.9862878918647766\n",
      "223 0.9861671328544617\n",
      "224 0.9860466122627258\n",
      "225 0.9859261512756348\n",
      "226 0.9858056902885437\n",
      "227 0.9856855273246765\n",
      "228 0.9855664968490601\n",
      "229 0.985445499420166\n",
      "230 0.9853256344795227\n",
      "231 0.9852054715156555\n",
      "232 0.9850857853889465\n",
      "233 0.9849656224250793\n",
      "234 0.984845757484436\n",
      "235 0.984725832939148\n",
      "236 0.984605610370636\n",
      "237 0.9844855070114136\n",
      "238 0.9843658208847046\n",
      "239 0.9842459559440613\n",
      "240 0.9841252565383911\n",
      "241 0.9840065240859985\n",
      "242 0.9838865399360657\n",
      "243 0.9837659597396851\n",
      "244 0.9836466908454895\n",
      "245 0.9835267066955566\n",
      "246 0.9834074974060059\n",
      "247 0.9832871556282043\n",
      "248 0.9831679463386536\n",
      "249 0.9830478429794312\n",
      "250 0.9829291105270386\n",
      "251 0.9828089475631714\n",
      "252 0.9826886057853699\n",
      "253 0.9825693368911743\n",
      "254 0.9824501276016235\n",
      "255 0.9823309183120728\n",
      "256 0.982210636138916\n",
      "257 0.9820919036865234\n",
      "258 0.9819722175598145\n",
      "259 0.9818528294563293\n",
      "260 0.9817337989807129\n",
      "261 0.9816145896911621\n",
      "262 0.9814961552619934\n",
      "263 0.9813764691352844\n",
      "264 0.9812563061714172\n",
      "265 0.981137752532959\n",
      "266 0.981019139289856\n",
      "267 0.9808996915817261\n",
      "268 0.9807804822921753\n",
      "269 0.9806615114212036\n",
      "270 0.9805420637130737\n",
      "271 0.9804237484931946\n",
      "272 0.9803048968315125\n",
      "273 0.9801858067512512\n",
      "274 0.9800665974617004\n",
      "275 0.9799472689628601\n",
      "276 0.9798280000686646\n",
      "277 0.9797091484069824\n",
      "278 0.9795905947685242\n",
      "279 0.9794713854789734\n",
      "280 0.9793530702590942\n",
      "281 0.9792336225509644\n",
      "282 0.9791153073310852\n",
      "283 0.978997528553009\n",
      "284 0.9788775444030762\n",
      "285 0.9787598848342896\n",
      "286 0.9786415100097656\n",
      "287 0.9785234332084656\n",
      "288 0.9784040451049805\n",
      "289 0.9782859086990356\n",
      "290 0.9781683683395386\n",
      "291 0.9780497550964355\n",
      "292 0.9779307246208191\n",
      "293 0.9778116345405579\n",
      "294 0.9776948094367981\n",
      "295 0.977575957775116\n",
      "296 0.9774576425552368\n",
      "297 0.9773387908935547\n",
      "298 0.9772218465805054\n",
      "299 0.9771031141281128\n",
      "300 0.9769845008850098\n",
      "301 0.9768670201301575\n",
      "302 0.9767483472824097\n",
      "303 0.9766305088996887\n",
      "304 0.9765124320983887\n",
      "305 0.976394772529602\n",
      "306 0.9762762784957886\n",
      "307 0.9761580228805542\n",
      "308 0.9760400652885437\n",
      "309 0.9759219884872437\n",
      "310 0.9758037328720093\n",
      "311 0.9756857752799988\n",
      "312 0.9755679368972778\n",
      "313 0.9754493832588196\n",
      "314 0.9753325581550598\n",
      "315 0.9752146005630493\n",
      "316 0.9750970005989075\n",
      "317 0.9749789237976074\n",
      "318 0.9748612642288208\n",
      "319 0.9747425317764282\n",
      "320 0.9746249914169312\n",
      "321 0.9745074510574341\n",
      "322 0.9743905067443848\n",
      "323 0.9742729067802429\n",
      "324 0.9741555452346802\n",
      "325 0.9740369915962219\n",
      "326 0.973920464515686\n",
      "327 0.9738028645515442\n",
      "328 0.9736849665641785\n",
      "329 0.9735681414604187\n",
      "330 0.9734503626823425\n",
      "331 0.9733330607414246\n",
      "332 0.9732156991958618\n",
      "333 0.9730985760688782\n",
      "334 0.9729815721511841\n",
      "335 0.9728633761405945\n",
      "336 0.9727460145950317\n",
      "337 0.9726292490959167\n",
      "338 0.9725130200386047\n",
      "339 0.9723955988883972\n",
      "340 0.9722775220870972\n",
      "341 0.9721605181694031\n",
      "342 0.9720438718795776\n",
      "343 0.9719259142875671\n",
      "344 0.9718092679977417\n",
      "345 0.971691906452179\n",
      "346 0.9715753793716431\n",
      "347 0.9714582562446594\n",
      "348 0.9713414311408997\n",
      "349 0.9712244868278503\n",
      "350 0.9711083173751831\n",
      "351 0.9709912538528442\n",
      "352 0.9708746075630188\n",
      "353 0.9707580804824829\n",
      "354 0.9706419706344604\n",
      "355 0.9705255627632141\n",
      "356 0.9704083204269409\n",
      "357 0.9702922701835632\n",
      "358 0.9701749682426453\n",
      "359 0.9700583219528198\n",
      "360 0.9699423909187317\n",
      "361 0.9698251485824585\n",
      "362 0.9697095155715942\n",
      "363 0.9695927500724792\n",
      "364 0.969476580619812\n",
      "365 0.9693595767021179\n",
      "366 0.9692443013191223\n",
      "367 0.969127357006073\n",
      "368 0.969011664390564\n",
      "369 0.9688952565193176\n",
      "370 0.9687789678573608\n",
      "371 0.9686628580093384\n",
      "372 0.9685470461845398\n",
      "373 0.968431293964386\n",
      "374 0.9683148264884949\n",
      "375 0.9681988954544067\n",
      "376 0.9680818319320679\n",
      "377 0.9679667353630066\n",
      "378 0.9678505063056946\n",
      "379 0.9677346348762512\n",
      "380 0.9676183462142944\n",
      "381 0.9675028920173645\n",
      "382 0.9673870205879211\n",
      "383 0.9672709703445435\n",
      "384 0.9671558141708374\n",
      "385 0.9670394062995911\n",
      "386 0.9669235348701477\n",
      "387 0.96680748462677\n",
      "388 0.9666925668716431\n",
      "389 0.9665764570236206\n",
      "390 0.9664610028266907\n",
      "391 0.9663456082344055\n",
      "392 0.9662299156188965\n",
      "393 0.966114342212677\n",
      "394 0.9659981727600098\n",
      "395 0.9658830761909485\n",
      "396 0.9657669067382812\n",
      "397 0.96565181016922\n",
      "398 0.9655356407165527\n",
      "399 0.9654216766357422\n",
      "400 0.965305507183075\n",
      "401 0.9651899337768555\n",
      "402 0.9650745391845703\n",
      "403 0.9649595022201538\n",
      "404 0.9648436307907104\n",
      "405 0.9647279977798462\n",
      "406 0.9646136164665222\n",
      "407 0.9644983410835266\n",
      "408 0.9643834233283997\n",
      "409 0.964267373085022\n",
      "410 0.9641531109809875\n",
      "411 0.9640372395515442\n",
      "412 0.9639221429824829\n",
      "413 0.9638068079948425\n",
      "414 0.9636920094490051\n",
      "415 0.9635763168334961\n",
      "416 0.9634615778923035\n",
      "417 0.9633466005325317\n",
      "418 0.9632312059402466\n",
      "419 0.9631161689758301\n",
      "420 0.9630012512207031\n",
      "421 0.9628864526748657\n",
      "422 0.9627712965011597\n",
      "423 0.9626561403274536\n",
      "424 0.9625412821769714\n",
      "425 0.9624269604682922\n",
      "426 0.9623112678527832\n",
      "427 0.9621971845626831\n",
      "428 0.9620822072029114\n",
      "429 0.9619683027267456\n",
      "430 0.9618531465530396\n",
      "431 0.9617388844490051\n",
      "432 0.9616238474845886\n",
      "433 0.9615090489387512\n",
      "434 0.9613951444625854\n",
      "435 0.9612811803817749\n",
      "436 0.961165726184845\n",
      "437 0.9610525369644165\n",
      "438 0.9609382748603821\n",
      "439 0.9608240127563477\n",
      "440 0.9607096910476685\n",
      "441 0.9605956077575684\n",
      "442 0.9604816436767578\n",
      "443 0.9603669047355652\n",
      "444 0.9602535963058472\n",
      "445 0.9601396322250366\n",
      "446 0.9600263833999634\n",
      "447 0.9599118232727051\n",
      "448 0.9597979784011841\n",
      "449 0.9596840143203735\n",
      "450 0.9595705270767212\n",
      "451 0.9594562649726868\n",
      "452 0.9593428373336792\n",
      "453 0.9592289924621582\n",
      "454 0.9591148495674133\n",
      "455 0.959000289440155\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "456 0.9588869214057922\n",
      "457 0.9587734937667847\n",
      "458 0.9586593508720398\n",
      "459 0.9585461616516113\n",
      "460 0.9584329724311829\n",
      "461 0.9583184123039246\n",
      "462 0.9582053422927856\n",
      "463 0.9580910801887512\n",
      "464 0.9579782485961914\n",
      "465 0.9578644633293152\n",
      "466 0.9577502012252808\n",
      "467 0.9576373100280762\n",
      "468 0.9575239419937134\n",
      "469 0.957409679889679\n",
      "470 0.957297146320343\n",
      "471 0.957183837890625\n",
      "472 0.9570702314376831\n",
      "473 0.9569562673568726\n",
      "474 0.9568436741828918\n",
      "475 0.9567298889160156\n",
      "476 0.956616222858429\n",
      "477 0.9565026164054871\n",
      "478 0.9563896059989929\n",
      "479 0.9562757611274719\n",
      "480 0.9561635255813599\n",
      "481 0.9560497999191284\n",
      "482 0.9559370279312134\n",
      "483 0.9558240175247192\n",
      "484 0.9557105302810669\n",
      "485 0.9555975794792175\n",
      "486 0.9554837346076965\n",
      "487 0.9553712606430054\n",
      "488 0.9552586674690247\n",
      "489 0.955145537853241\n",
      "490 0.955032229423523\n",
      "491 0.954919159412384\n",
      "492 0.9548062086105347\n",
      "493 0.9546929597854614\n",
      "494 0.9545804858207703\n",
      "495 0.9544674754142761\n",
      "496 0.9543545842170715\n",
      "497 0.9542415738105774\n",
      "498 0.9541285634040833\n",
      "499 0.9540163278579712\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# N is batch size; D_in is input dimension;\n",
    "# H is hidden dimension; D_out is output dimension.\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# Create random Tensors to hold inputs and outputs\n",
    "x = torch.randn(N, D_in)\n",
    "y = torch.randn(N, D_out)\n",
    "\n",
    "# Use the nn package to define our model as a sequence of layers. nn.Sequential\n",
    "# is a Module which contains other Modules, and applies them in sequence to\n",
    "# produce its output. Each Linear Module computes output from input using a\n",
    "# linear function, and holds internal Tensors for its weight and bias.\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(D_in, H),\n",
    "#     torch.nn.ReLU(),\n",
    "#     torch.nn.Linear(H, D_out),\n",
    ")\n",
    "\n",
    "# The nn package also contains definitions of popular loss functions; in this\n",
    "# case we will use Mean Squared Error (MSE) as our loss function.\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "\n",
    "learning_rate = 1e-4\n",
    "for t in range(500):\n",
    "    # Forward pass: compute predicted y by passing x to the model. Module objects\n",
    "    # override the __call__ operator so you can call them like functions. When\n",
    "    # doing so you pass a Tensor of input data to the Module and it produces\n",
    "    # a Tensor of output data.\n",
    "    y_pred = model(x)\n",
    "\n",
    "    # Compute and print loss. We pass Tensors containing the predicted and true\n",
    "    # values of y, and the loss function returns a Tensor containing the\n",
    "    # loss.\n",
    "    loss = loss_fn(y_pred, y)\n",
    "    print(t, loss.item())\n",
    "\n",
    "    # Zero the gradients before running the backward pass.\n",
    "    model.zero_grad()\n",
    "\n",
    "    # Backward pass: compute gradient of the loss with respect to all the learnable\n",
    "    # parameters of the model. Internally, the parameters of each Module are stored\n",
    "    # in Tensors with requires_grad=True, so this call will compute gradients for\n",
    "    # all learnable parameters in the model.\n",
    "    loss.backward()\n",
    "\n",
    "    # Update the weights using gradient descent. Each parameter is a Tensor, so\n",
    "    # we can access its gradients like we did before.\n",
    "    with torch.no_grad():\n",
    "        for param in model.parameters():\n",
    "            param -= learning_rate * param.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
